{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(loc):\n",
    "    ans = {}\n",
    "    dirs = os.listdir(loc)\n",
    "    for ind in dirs:\n",
    "        doc = []\n",
    "        files = os.listdir(loc+'/'+ind)\n",
    "        for f in files:\n",
    "            floc = loc+'/'+ind+'/'+f\n",
    "            with open(floc, 'r') as content_file:\n",
    "                content = content_file.read()\n",
    "                content=str(content).decode('UTF-8', 'ignore')\n",
    "                doc.append(content)\n",
    "        ans[ind]=doc\n",
    "    return ans\n",
    "val = get_data('q2data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2,max_features= 1000,analyzer='word',stop_words='english')\n",
    "print val.keys()\n",
    "print \"assd\"\n",
    "for d in val.keys():\n",
    "    docs = val[d]\n",
    "    # Train the vectorizer on the descriptions\n",
    "    vectorizer = vectorizer.fit(docs)\n",
    "for d in val.keys():\n",
    "    docs = val[d]\n",
    "    # Convert descriptions to feature vectors\n",
    "    tfidf = vectorizer.transform(docs)\n",
    "    tfidf=tfidf.todense()\n",
    "    tfidf=tfidf.tolist()\n",
    "    print tfidf[0]\n",
    "    #print X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bow(loc):\n",
    "    ans = {}\n",
    "    dirs = os.listdir(loc)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    df = pd.DataFrame(columns=['label'])\n",
    "    counter=0\n",
    "    for ind in dirs:\n",
    "        files = os.listdir(loc+'/'+ind)\n",
    "        for f in files:\n",
    "            df.loc[counter]=0\n",
    "            df.loc[counter, 'label'] = ind\n",
    "            floc = loc+'/'+ind+'/'+f\n",
    "            with open(floc, 'r') as content_file:\n",
    "                content = content_file.read()\n",
    "                content=str(content).decode('UTF-8', 'ignore').lower()\n",
    "                word_tokens = word_tokenize(content)\n",
    "                filtered_sentence = [ps.stem(w) for w in word_tokens if not w in stop_words and not ps.stem(w) in stop_words and w.isalnum()]\n",
    "                for word in filtered_sentence:\n",
    "                    if word not in df.columns:\n",
    "                        df[word]=0\n",
    "                    df.loc[counter, word] += 1\n",
    "            counter += 1\n",
    "    return df\n",
    "\n",
    "def add_labels(loc, df):\n",
    "    ans = {}\n",
    "    dirs = os.listdir(loc)\n",
    "    label = []\n",
    "    for ind in dirs:\n",
    "        files = os.listdir(loc+'/'+ind)\n",
    "        for f in files:\n",
    "            label.append(ind)\n",
    "    label = np.array(label)\n",
    "    df['label'] = label\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_bow = get_bow('q2data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_bow.to_csv(\"q3_bow.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17669"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_bow.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bow = final_bow.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bow['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bow = copy_bow.drop('doc_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_labels('q2data/train', dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr.to_csv(\"q3_bow_final.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_ds_without_label = dr.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = new_ds_without_label.values\n",
    "words = list(new_ds_without_label.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"ass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr = pd.read_csv(\"q3_bow_final.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.asmatrix(dr['label']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['docid'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-915b20bee491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'docid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/satyam/.local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/satyam/.local/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3624\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['docid'] not contained in axis"
     ]
    }
   ],
   "source": [
    "dr = dr.drop('docid', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-40e3936c93c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer()\n",
    "r = tf_transformer.fit_transform(val)\n",
    "np.save('tfidf_1', r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_load = np.load('tfidf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<1760x17667 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 248752 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = sc.sparse.csc_matrix(vectors_load.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1760, 17667)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat = mt.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.02452894,  0.03863099,  0.05152728, ...,  0.        ,\n",
       "          0.        ,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epoch = 5\n",
    "diff_epoch = 5\n",
    "max_epoch = 20\n",
    "\n",
    "def VanillaPerceptron(array, epoch=1):\n",
    "    n = array.shape[1]\n",
    "    w = np.zeros(n-1)\n",
    "    b = 0\n",
    "    counter=0\n",
    "    for t in range(epoch):\n",
    "        for x in array:\n",
    "            counter+=1\n",
    "            y_pred = np.dot(x[:-1],w) + b\n",
    "            if (y_pred * x[-1]) <= 0:\n",
    "                w += x[:-1] * x[-1]\n",
    "                b += x[-1]\n",
    "            #print (counter, w, b)\n",
    "    return np.append(np.copy(w),b)\n",
    "\n",
    "def makeitsingular(number, k):\n",
    "    if number == k:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def multiclassPerceptron(array, epoch, numclasses):\n",
    "    label = array[:,-1].copy()\n",
    "    w = []\n",
    "    for k in range(numclasses):\n",
    "        clas = k+1\n",
    "        print np.unique(label)\n",
    "        arr = np.array([makeitsingular(t, clas) for t in label])\n",
    "        array[:, -1] = arr\n",
    "        w.append(VanillaPerceptron(array, epoch))\n",
    "    return w\n",
    "    \n",
    "    \n",
    "def predictVanilla(model, data):\n",
    "    y = data.copy()\n",
    "    y = np.append(y,1)\n",
    "    minh = -10000000000007\n",
    "    pred = 1\n",
    "    counter = 0\n",
    "    for m in model:\n",
    "        counter += 1\n",
    "        val = np.dot(m,y)\n",
    "        if minh < val:\n",
    "            minh = val\n",
    "            pred = counter\n",
    "    return pred\n",
    "\n",
    "def merge_arrays(arr):\n",
    "    out = np.array([])\n",
    "    #print out.shape\n",
    "    for r in arr:\n",
    "        if r.shape[0]==0:\n",
    "            pass\n",
    "        elif out.shape[0]==0:\n",
    "            out = r\n",
    "        else:\n",
    "            out = np.concatenate((out, r), axis=0)\n",
    "    #print out.shape\n",
    "    return out\n",
    "\n",
    "def score(test_real, test_pred):\n",
    "    total = float(len(test_real))\n",
    "    same = 0.0\n",
    "    for i in range(len(test_real)):\n",
    "        if test_real[i]==test_pred[i]:\n",
    "            same += 1.0\n",
    "    return float(same/total)\n",
    "\n",
    "def vanilla_perceptron_model(train, test, epoch=1):\n",
    "    model = multiclassPerceptron(train, epoch, 5)\n",
    "    test_in = np.delete(test, -1, axis=1)\n",
    "    test_out = []\n",
    "    for val in test_in:\n",
    "        pred = predictVanilla(model, val)\n",
    "        test_out.append(pred)\n",
    "    test_real = test[:,-1]\n",
    "    print test_real, test_out\n",
    "    return score(test_real, test_out)\n",
    "\n",
    "def sign(num):\n",
    "    if num<=0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def perceptron_cross_valid(arr, string):\n",
    "    k = 10\n",
    "    r = np.copy(arr)\n",
    "    np.random.shuffle(r)\n",
    "    split_arr = np.asarray(np.array_split(r, k))\n",
    "    acc_mat = []\n",
    "    for i in xrange(min_epoch, max_epoch+1, diff_epoch):\n",
    "        ans_arr = []\n",
    "        for j in range(k):\n",
    "            test_arr = split_arr[j]\n",
    "            train_arr = merge_arrays(split_arr[:j])\n",
    "            t = merge_arrays(split_arr[j+1:])\n",
    "            train_arr = merge_arrays([train_arr,t])\n",
    "            if string==\"voted\":\n",
    "                accuracy = voted_perceptron_model(train_arr, test_arr, i)\n",
    "            elif string==\"vanilla\":\n",
    "                accuracy = vanilla_perceptron_model(train_arr, test_arr, i)\n",
    "            ans_arr.append(accuracy)\n",
    "        larr = np.array(ans_arr)\n",
    "        print (\"Epoch {0} -> {1} +/- {2}\").format(i, larr.mean(), 2*larr.std())\n",
    "        acc_mat.append(larr.mean())\n",
    "    return acc_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.append(tfidf_mat, label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1760, 17668)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = perceptron_cross_valid(np.asarray(dataset), \"vanilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ...,  0.11166979,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(dataset)[:,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = multiclassPerceptron(np.asarray(dataset).copy(),1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.74829545454545454,\n",
       " 0.7488636363636364,\n",
       " 0.7488636363636364,\n",
       " 0.7488636363636364]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
